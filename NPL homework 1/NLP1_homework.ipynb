{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dH7qx_irU4Y8"
   },
   "source": [
    "###ML1_1: \n",
    "https://www.hackerrank.com/challenges/capturing-non-capturing-groups/problem?isFullScreen=true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Введите строку: okok\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Совпадение найдено!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# Ввод строки\n",
    "test_string = input(\"Введите строку:\")\n",
    "\n",
    "# Регулярное выражение для поиска двух или более \"ok\" подряд\n",
    "pattern = r'(ok){2,}'\n",
    "\n",
    "# Проверка на совпадение\n",
    "if re.search(pattern, test_string):\n",
    "    print(\"Совпадение найдено!\")\n",
    "else:\n",
    "    print(\"Совпадение не найдено.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###ML1_2: \n",
    "https://www.hackerrank.com/challenges/branch-reset-groups/problem?isFullScreen=true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Введите строку:  12.34.56.78\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Совпадение найдено!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# Ввод строки\n",
    "test_string = input(\"Введите строку: \")\n",
    "\n",
    "# Регулярное выражение для проверки строки\n",
    "pattern = r'^\\d{2}([-:.]|---)\\d{2}\\1\\d{2}\\1\\d{2}$'\n",
    "\n",
    "# Проверка на совпадение\n",
    "if re.match(pattern, test_string):\n",
    "    print(\"Совпадение найдено!\")\n",
    "else:\n",
    "    print(\"Совпадение не найдено.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###ML1_3: \n",
    "https://www.hackerrank.com/challenges/detect-html-links/problem?isFullScreen=true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "  2\n",
      " <p><a href=\"https://www.hackerrank.com\">Example Link</a></p>\n",
      " <div><a href=\"https://www.hackerrank.com/another\">Another Link</a></div>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.hackerrank.com,Example Link\n",
      "https://www.hackerrank.com/another,Another Link\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import sys\n",
    "\n",
    "# Чтение количества строк HTML-фрагмента\n",
    "n = int(input())\n",
    "\n",
    "# Чтение всех строк HTML-фрагмента\n",
    "html_content = ''.join([input().strip() for _ in range(n)])\n",
    "\n",
    "# Создание объекта BeautifulSoup для разбора HTML\n",
    "soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "# Проход по всем тегам <a>\n",
    "for link in soup.find_all('a', href=True):\n",
    "    # Извлечение ссылки\n",
    "    href = link['href'].strip()\n",
    "    # Извлечение текста внутри тега, очищенного от вложенных тегов\n",
    "    text = link.get_text().strip()\n",
    "    # Печать результата\n",
    "    print(f\"{href},{text}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xJfkstKpqsXp"
   },
   "source": [
    "###ML1_4: \n",
    "Реализовать stemming, lemmatization & BoW на следующем датасете: https://cloud.mail.ru/public/Z4L3/vB8GcgTtK (Russian Toxic-abuse comments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                             comment  \\\n",
      "0               Верблюдов-то за что? Дебилы, бл...\\n   \n",
      "1  Хохлы, это отдушина затюканого россиянина, мол...   \n",
      "2                          Собаке - собачья смерть\\n   \n",
      "3  Страницу обнови, дебил. Это тоже не оскорблени...   \n",
      "4  тебя не убедил 6-страничный пдф в том, что Скр...   \n",
      "\n",
      "                                     stemmed_comment  \\\n",
      "0                      верблюдов то за что дебилы бл   \n",
      "1  хохлы это отдушина затюканого россиянина мол в...   \n",
      "2                              собаке собачья смерть   \n",
      "3  страницу обнови дебил это тоже не оскорбление ...   \n",
      "4  тебя не убедил 6 страничный пдф в том что скри...   \n",
      "\n",
      "                                  lemmatized_comment  \n",
      "0                      верблюдов то за что дебилы бл  \n",
      "1  хохлы это отдушина затюканого россиянина мол в...  \n",
      "2                              собаке собачья смерть  \n",
      "3  страницу обнови дебил это тоже не оскорбление ...  \n",
      "4  тебя не убедил 6 страничный пдф в том что скри...  \n",
      "   00  000  0015  003  0036  005  00х  01  013  02  ...  ёлка  ёлку  ёмаё  \\\n",
      "0   0    0     0    0     0    0    0   0    0   0  ...     0     0     0   \n",
      "1   0    0     0    0     0    0    0   0    0   0  ...     0     0     0   \n",
      "2   0    0     0    0     0    0    0   0    0   0  ...     0     0     0   \n",
      "3   0    0     0    0     0    0    0   0    0   0  ...     0     0     0   \n",
      "4   0    0     0    0     0    0    0   0    0   0  ...     0     0     0   \n",
      "\n",
      "   ёмкостей  ёмкости  ёмкость  ёмкостью  ёпта  ёта  ётавских  \n",
      "0         0        0        0         0     0    0         0  \n",
      "1         0        0        0         0     0    0         0  \n",
      "2         0        0        0         0     0    0         0  \n",
      "3         0        0        0         0     0    0         0  \n",
      "4         0        0        0         0     0    0         0  \n",
      "\n",
      "[5 rows x 68387 columns]\n"
     ]
    }
   ],
   "source": [
    "# Импорт необходимых библиотек\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.stem import SnowballStemmer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import spacy\n",
    "import string\n",
    "import os\n",
    "\n",
    "# 1. Установка и загрузка необходимых ресурсов\n",
    "\n",
    "# Загрузка необходимых пакетов NLTK\n",
    "nltk_packages = ['punkt', 'stopwords']\n",
    "for package in nltk_packages:\n",
    "    try:\n",
    "        nltk.data.find(f'tokenizers/{package}')\n",
    "    except LookupError:\n",
    "        nltk.download(package)\n",
    "\n",
    "# Загрузка русских стоп-слов из NLTK\n",
    "from nltk.corpus import stopwords\n",
    "russian_stopwords = set(stopwords.words(\"russian\"))\n",
    "\n",
    "# Загрузка модели spaCy для русского языка\n",
    "try:\n",
    "    nlp = spacy.load(\"ru_core_news_sm\")\n",
    "except OSError:\n",
    "    print(\"Модель spaCy 'ru_core_news_sm' не найдена. Устанавливаем...\")\n",
    "    os.system(\"python -m spacy download ru_core_news_sm\")\n",
    "    nlp = spacy.load(\"ru_core_news_sm\")\n",
    "\n",
    "# 2. Загрузка данных\n",
    "\n",
    "# Путь к файлу\n",
    "file_path = r'C:\\Users\\vtako\\Desktop\\Project\\NPL homework 1\\labeled.csv'\n",
    "\n",
    "# Загрузка датасета\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Проверка первых строк датасета\n",
    "print(\"Первые строки датасета:\")\n",
    "print(df.head())\n",
    "\n",
    "# 3. Предобработка текста\n",
    "\n",
    "def preprocess(text):\n",
    "    if pd.isnull(text):\n",
    "        return \"\"\n",
    "    # Приведение к нижнему регистру\n",
    "    text = text.lower()\n",
    "    # Удаление пунктуации и цифр\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation + string.digits))\n",
    "    return text\n",
    "\n",
    "# Применение предобработки к колонке \"comment\"\n",
    "df['comment_clean'] = df['comment'].apply(preprocess)\n",
    "\n",
    "# 4. Реализация Стемминга\n",
    "\n",
    "# Инициализация стеммера для русского языка\n",
    "stemmer = SnowballStemmer(\"russian\")\n",
    "\n",
    "def stemming(text):\n",
    "    if not text:\n",
    "        return \"\"\n",
    "    # Токенизация с использованием spaCy\n",
    "    doc = nlp(text)\n",
    "    tokens = [token.text for token in doc if not token.is_punct and not token.is_space]\n",
    "    # Применение стемминга к каждому слову\n",
    "    stemmed_words = [stemmer.stem(word) for word in tokens]\n",
    "    return \" \".join(stemmed_words)\n",
    "\n",
    "# Применение стемминга к колонке \"comment_clean\"\n",
    "df['comment_stemmed'] = df['comment_clean'].apply(stemming)\n",
    "\n",
    "# Просмотр результатов стемминга\n",
    "print(\"\\nПример стемминга:\")\n",
    "print(df[['comment', 'comment_stemmed']].head())\n",
    "\n",
    "# 5. Реализация Лемматизации\n",
    "\n",
    "def lemmatization(text):\n",
    "    if not text:\n",
    "        return \"\"\n",
    "    doc = nlp(text)\n",
    "    # Собираем леммы, исключая знаки препинания и пробелы\n",
    "    lemmas = [token.lemma_ for token in doc if not token.is_punct and not token.is_space]\n",
    "    return \" \".join(lemmas)\n",
    "\n",
    "# Применение лемматизации к колонке \"comment_clean\"\n",
    "df['comment_lemmatized'] = df['comment_clean'].apply(lemmatization)\n",
    "\n",
    "# Просмотр результатов лемматизации\n",
    "print(\"\\nПример лемматизации:\")\n",
    "print(df[['comment', 'comment_lemmatized']].head())\n",
    "\n",
    "# 6. Создание Bag of Words (BoW)\n",
    "\n",
    "# Инициализация CountVectorizer с учётом русских стоп-слов\n",
    "vectorizer = CountVectorizer(stop_words=russian_stopwords)\n",
    "\n",
    "# Создание BoW на основе лемматизированных комментариев\n",
    "X_bow = vectorizer.fit_transform(df['comment_lemmatized'])\n",
    "\n",
    "# Получение матрицы признаков в виде DataFrame\n",
    "bow_df = pd.DataFrame(X_bow.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "\n",
    "# Ограничение количества признаков (опционально)\n",
    "# Например, оставить только 1000 наиболее частотных слов\n",
    "# vectorizer = CountVectorizer(stop_words=russian_stopwords, max_features=1000)\n",
    "# X_bow = vectorizer.fit_transform(df['comment_lemmatized'])\n",
    "# bow_df = pd.DataFrame(X_bow.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "\n",
    "# Добавление BoW к исходному датасету\n",
    "df = pd.concat([df, bow_df], axis=1)\n",
    "\n",
    "# Просмотр обновленного датасета\n",
    "print(\"\\nОбновленный датасет с BoW:\")\n",
    "print(df.head())\n",
    "\n",
    "# 7. Сохранение обработанного датасета (опционально)\n",
    "\n",
    "processed_file_path = r'C:\\Users\\vtako\\Desktop\\Project\\NPL homework 1\\processed_labeled.csv'\n",
    "df.to_csv(processed_file_path, index=False)\n",
    "print(f\"\\nОбработанный датасет сохранён по пути: {processed_file_path}\")\n",
    "\n",
    "# 8. Дополнительные рекомендации\n",
    "\n",
    "# Удаление пропущенных значений (если необходимо)\n",
    "# Например, заполнение пропусков пустыми строками\n",
    "df['comment'] = df['comment'].fillna('')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "NLP1_homework",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
